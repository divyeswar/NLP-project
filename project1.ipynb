{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936a602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import sklearn\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier,XGBRFClassifier\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Embedding,Bidirectional,LSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "414adafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0    acl.id     X1  \\\n",
      "0               1      acl1      1   \n",
      "1               2     acl10      2   \n",
      "2               3    acl100      3   \n",
      "3               4   acl1000      4   \n",
      "4               5  acl10000      5   \n",
      "...           ...       ...    ...   \n",
      "41139      117100   acl9974  41228   \n",
      "41140      118100  acl11574   1752   \n",
      "41141      119100  acl13446   3832   \n",
      "41142        1205  acl13457   3844   \n",
      "41143      121100  acl32095  24553   \n",
      "\n",
      "                                                    text    label  \\\n",
      "0                                    dalits are lowlives     hate   \n",
      "1                 gay people are a burden to our society     hate   \n",
      "2                                  Arabs are not welcome     hate   \n",
      "3      I'm not saying we should actually eliminate he...     hate   \n",
      "4                           bananas are for black people     hate   \n",
      "...                                                  ...      ...   \n",
      "41139                     Indian culture is so backwards     hate   \n",
      "41140  Women don't want equality, they just want to b...     hate   \n",
      "41141                                         fuck covid  nothate   \n",
      "41142                           This computer is garbage  nothate   \n",
      "41143                 The only good Muslim is a dead one     hate   \n",
      "\n",
      "              type    target     level  split  round.base   annotator round  \\\n",
      "0         notgiven  notgiven  original  train           1  GNZuCtwed3     1   \n",
      "1         notgiven  notgiven  original  train           1  GNZuCtwed3     1   \n",
      "2         notgiven  notgiven  original  train           1  vDe7GN0NrL     1   \n",
      "3         notgiven  notgiven  original  train           1  oemYWm1Tjg     1   \n",
      "4         notgiven  notgiven  original   test           1  QiOKkCi7F8     1   \n",
      "...            ...       ...       ...    ...         ...         ...   ...   \n",
      "41139     notgiven  notgiven  original   test           1  GNZuCtwed3     1   \n",
      "41140   derogation       wom  original  train           2  CAgNlUizNm    2a   \n",
      "41141         none      none  original  train           2  CAgNlUizNm    2a   \n",
      "41142         none      none  original  train           2  eLGzdD8Tvb    2a   \n",
      "41143  threatening       mus  original  train           4  IBsVsBliwX    4a   \n",
      "\n",
      "      acl.id.matched  \n",
      "0                NaN  \n",
      "1                NaN  \n",
      "2                NaN  \n",
      "3                NaN  \n",
      "4                NaN  \n",
      "...              ...  \n",
      "41139            NaN  \n",
      "41140       acl11575  \n",
      "41141       acl13447  \n",
      "41142       acl13458  \n",
      "41143       acl32096  \n",
      "\n",
      "[41144 rows x 13 columns]\n",
      "       Unnamed: 0    acl.id     X1  \\\n",
      "0               1      acl1      1   \n",
      "1               2     acl10      2   \n",
      "2               3    acl100      3   \n",
      "3               4   acl1000      4   \n",
      "4               5  acl10000      5   \n",
      "...           ...       ...    ...   \n",
      "41139      117100   acl9974  41228   \n",
      "41140      118100  acl11574   1752   \n",
      "41141      119100  acl13446   3832   \n",
      "41142        1205  acl13457   3844   \n",
      "41143      121100  acl32095  24553   \n",
      "\n",
      "                                                    text label         type  \\\n",
      "0                                    dalits are lowlives     1     notgiven   \n",
      "1                 gay people are a burden to our society     1     notgiven   \n",
      "2                                  Arabs are not welcome     1     notgiven   \n",
      "3      I'm not saying we should actually eliminate he...     1     notgiven   \n",
      "4                           bananas are for black people     1     notgiven   \n",
      "...                                                  ...   ...          ...   \n",
      "41139                     Indian culture is so backwards     1     notgiven   \n",
      "41140  Women don't want equality, they just want to b...     1   derogation   \n",
      "41141                                         fuck covid     0         none   \n",
      "41142                           This computer is garbage     0         none   \n",
      "41143                 The only good Muslim is a dead one     1  threatening   \n",
      "\n",
      "         target     level  split  round.base   annotator round acl.id.matched  \n",
      "0      notgiven  original  train           1  GNZuCtwed3     1            NaN  \n",
      "1      notgiven  original  train           1  GNZuCtwed3     1            NaN  \n",
      "2      notgiven  original  train           1  vDe7GN0NrL     1            NaN  \n",
      "3      notgiven  original  train           1  oemYWm1Tjg     1            NaN  \n",
      "4      notgiven  original   test           1  QiOKkCi7F8     1            NaN  \n",
      "...         ...       ...    ...         ...         ...   ...            ...  \n",
      "41139  notgiven  original   test           1  GNZuCtwed3     1            NaN  \n",
      "41140       wom  original  train           2  CAgNlUizNm    2a       acl11575  \n",
      "41141      none  original  train           2  CAgNlUizNm    2a       acl13447  \n",
      "41142      none  original  train           2  eLGzdD8Tvb    2a       acl13458  \n",
      "41143       mus  original  train           4  IBsVsBliwX    4a       acl32096  \n",
      "\n",
      "[41144 rows x 13 columns]\n",
      "                                                 comment isHate\n",
      "0                                    dalits are lowlives      1\n",
      "1                 gay people are a burden to our society      1\n",
      "2                                  Arabs are not welcome      1\n",
      "3      I'm not saying we should actually eliminate he...      1\n",
      "4                           bananas are for black people      1\n",
      "...                                                  ...    ...\n",
      "41139                     Indian culture is so backwards      1\n",
      "41140  Women don't want equality, they just want to b...      1\n",
      "41141                                         fuck covid      0\n",
      "41142                           This computer is garbage      0\n",
      "41143                 The only good Muslim is a dead one      1\n",
      "\n",
      "[41144 rows x 2 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41144 entries, 0 to 41143\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   comment  41144 non-null  object\n",
      " 1   isHate   41144 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 643.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# dataset = panda.read_csv(\"Ethos_Dataset_Binary.csv\")\n",
    "# dataset\n",
    "df = pd.read_csv(\"Dynamically Generated Hate Dataset v0.2.3.csv\", delimiter=\",\", encoding='utf-8')\n",
    "print(df)\n",
    "df.loc[df[\"label\"] == 'hate', \"label\"] = 1\n",
    "df.loc[df[\"label\"] == 'nothate', \"label\"] = 0\n",
    "print(df)\n",
    "# df[\"isHate\"]=df[\"isHate\"].apply(pd.to_numeric)\n",
    "# df[\"isHate\"][df[\"isHate\"]>=0.5]=1\n",
    "# df[\"isHate\"][df[\"isHate\"]<0.5]=0\n",
    "# df.loc[df[\"isHate\"] >= 0.5, \"isHate\"] = 1\n",
    "# df.loc[df[\"isHate\"] < 0.5, \"isHate\"] = 0\n",
    "df=df[['text','label']]\n",
    "df.rename(columns={'text':'comment','label':'isHate'},inplace=True)\n",
    "print(df)\n",
    "df[\"isHate\"]=df[\"isHate\"].apply(pd.to_numeric)\n",
    "df.to_csv('file1.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76a32504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                      dalits are lowlives\n",
      "1                   gay people are a burden to our society\n",
      "2                                    Arabs are not welcome\n",
      "3        I'm not saying we should actually eliminate he...\n",
      "4                             bananas are for black people\n",
      "                               ...                        \n",
      "41139                       Indian culture is so backwards\n",
      "41140    Women don't want equality, they just want to b...\n",
      "41141                                           fuck covid\n",
      "41142                             This computer is garbage\n",
      "41143                   The only good Muslim is a dead one\n",
      "Name: comment, Length: 41144, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## 1. Removal of punctuation and capitlization\n",
    "## 2. Tokenizing\n",
    "## 3. Removal of stopwords\n",
    "## 4. Stemming\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(tweet):  \n",
    "    \n",
    "    # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    tweet_space = tweet.str.replace(regex_pat, ' ')\n",
    "\n",
    "    # removal of @name[mention]\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    tweet_name = tweet_space.str.replace(regex_pat, '')\n",
    "\n",
    "    # removal of links[https://abc.com]\n",
    "    giant_url_regex =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    tweets = tweet_name.str.replace(giant_url_regex, '')\n",
    "    \n",
    "    # removal of punctuations and numbers\n",
    "    punc_remove = tweets.str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    # remove whitespace with a single space\n",
    "    newtweet=punc_remove.str.replace(r'\\s+', ' ')\n",
    "    # remove leading and trailing whitespace\n",
    "    newtweet=newtweet.str.replace(r'^\\s+|\\s+?$','')\n",
    "    # replace normal numbers with numbr\n",
    "    newtweet=newtweet.str.replace(r'\\d+(\\.\\d+)?','numbr')\n",
    "    # removal of capitalization\n",
    "    tweet_lower = newtweet.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    # removal of stopwords\n",
    "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    # stemming of the tweets\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "#     print(tokenized_tweet)\n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "#         print(tokenized_tweet[i])\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p\n",
    "print(df.comment)\n",
    "processed_tweets = preprocess(df.comment)   \n",
    "\n",
    "df['processed_tweets'] = processed_tweets\n",
    "df = df.sample(frac = 1)\n",
    "# print(df[[\"comment\",\"processed_tweets\"]].head(10))\n",
    "# print(df[\"isHate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87217a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()                      \n",
    "word_tokenizer.fit_on_texts(df[\"processed_tweets\"])\n",
    "brown_tagged_words=df[\"processed_tweets\"]\n",
    "brown_tagged_words_encoded = word_tokenizer.texts_to_sequences(df[\"processed_tweets\"])\n",
    "# print(brown_tagged_words_encoded)\n",
    "brown_tagged_vect=np.array(df[\"isHate\"])\n",
    "# print(brown_tagged_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1833f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38018    0\n",
      "16336    0\n",
      "19877    0\n",
      "38909    1\n",
      "32217    1\n",
      "        ..\n",
      "40364    0\n",
      "4933     1\n",
      "6179     0\n",
      "17414    1\n",
      "21838    1\n",
      "Name: isHate, Length: 41144, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"isHate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b99956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence: 212\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(seq) for seq in brown_tagged_words_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b267a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = max(lengths)\n",
    "brown_tagged_words_padded = pad_sequences(brown_tagged_words_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "print(len(brown_tagged_words_padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67da43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((word_tokenizer.word_index) )\n",
    "path = \"GoogleNews-vectors-negative300.bin\"\n",
    "word2vec= KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "EMBEDDING_SIZE  = 300\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "word2id = word_tokenizer.word_index\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "58c8b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(word_tokenizer.word_index))\n",
    "# print(word_tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e76989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26332, 212) (26332,)\n",
      "Epoch 1/7\n",
      " 61/103 [================>.............] - ETA: 37s - loss: 2.1024 - binary_accuracy: 0.6162"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 47\u001b[0m\n\u001b[0;32m     42\u001b[0m network\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     44\u001b[0m network\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# Cross-entropy\u001b[39;00m\n\u001b[0;32m     45\u001b[0m             optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-3\u001b[39m), \u001b[38;5;66;03m# Root Mean Square Propagation\u001b[39;00m\n\u001b[0;32m     46\u001b[0m             metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 47\u001b[0m trainng \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_validation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m loss,accuracy \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mevaluate(X_test,Y_test,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "len_brown_data=len(brown_tagged_words_padded)/5\n",
    "avg_loss=0.0\n",
    "avg_acc=0.0\n",
    "confusion_mat=[]\n",
    "per_pos_acc=[]\n",
    "Y_pred=[]\n",
    "Y_true=[]\n",
    "# temp = list(zip(brown_tagged_words_padded,brown_tagged_vect ))\n",
    "# random.shuffle(temp)\n",
    "# brown_tagged_words_padded,brown_tagged_vect= zip(*temp)\n",
    "# # res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "# brown_tagged_words_padded,brown_tagged_vect = list(brown_tagged_words_padded), list(brown_tagged_vect)\n",
    "# print(brown_tagged_vect)\n",
    "# print(len(brown_tagged_words_padded))\n",
    "# for i in range(5):\n",
    "# first_part=int(i*(len_brown_data))\n",
    "# second_part=int((i+1)*len_brown_data)\n",
    "# X_train=np.concatenate((brown_tagged_words_padded[:first_part],brown_tagged_words_padded[second_part:]),axis=0)\n",
    "# X_test =brown_tagged_words_padded[first_part:second_part]\n",
    "# Y_train=np.concatenate((brown_tagged_vect[:first_part],brown_tagged_vect[second_part:]),axis=0)\n",
    "# Y_test =brown_tagged_vect[first_part:second_part]\n",
    "\n",
    "#     Y_test_enc=brown_tagged_tags_encoded[first_part:second_part]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(brown_tagged_words_padded, brown_tagged_vect, test_size=0.2, random_state=42)\n",
    "X_train,X_validation=train_test_split(X_train,train_size=0.80,test_size=0.20,random_state = 50)\n",
    "Y_train,Y_validation=train_test_split(Y_train,train_size=0.80,test_size=0.20,random_state = 50)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "#     print(X_validation.shape)\n",
    "network = models.Sequential() ## starting the ffnn\n",
    "network.add(Embedding(input_dim     = VOCABULARY_SIZE,\n",
    "                         output_dim    = EMBEDDING_SIZE,\n",
    "                         input_length  = MAX_SEQ_LENGTH,\n",
    "#                          weights       = [embedding_weights],\n",
    "                         ))\n",
    "network.add(Bidirectional(LSTM(16))) \n",
    "#     network.add(layers.Dense(units=100))\n",
    "#     network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "#     network.add(layers.Dense(units=1,activation='relu'))\n",
    "network.add(layers.Dense(512, kernel_regularizer='l1'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-3), # Root Mean Square Propagation\n",
    "            metrics=['binary_accuracy'])\n",
    "trainng = network.fit(X_train,Y_train,batch_size=256,epochs=7,validation_data=(X_validation, Y_validation))\n",
    "loss,accuracy = network.evaluate(X_test,Y_test,verbose=1)\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "avg_loss=avg_loss+loss\n",
    "avg_acc=avg_acc+accuracy\n",
    "pred=network.predict(X_test)\n",
    "print(pred)\n",
    "print(Y_test)\n",
    "# print(avg_acc/5)\n",
    "#     print(avg_acc)\n",
    "#     indices=np.argmax(pred, axis=2)\n",
    "    #not_zero=indices[indices != 0]\n",
    "#     Y_pred=indices \n",
    "#     Y_true=brown_tagged_tags_padded[first_part:second_part]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8170cdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q laserembeddings laserembeddings[zh] laserembeddings[ja]\n",
    "# !pip install -q ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176d9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import html\n",
    "import laserembeddings\n",
    "from fastcache import clru_cache\n",
    "from laserembeddings import Laser\n",
    "from typing import List, Union\n",
    "from urllib.parse import unquote\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c537c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# DOCS: https://github.com/facebookresearch/LASER/blob/master/install_models.sh\n",
    "\n",
    "mkdir -p models/laser/\n",
    "# for FILE in bilstm.eparl21.2018-11-19.pt eparl21.fcodes eparl21.fvocab bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\n",
    "for FILE in bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\n",
    "    wget -cq https://dl.fbaipublicfiles.com/laser/models/$FILE -O models/laser/$FILE\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c26bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"laser\": {\n",
    "        \"base_dir\":  \"./models/laser\",\n",
    "        \"bpe_codes\": \"./models/laser/93langs.fcodes\",\n",
    "        \"bpe_vocab\": \"./models/laser/93langs.fvocab\",\n",
    "        \"encoder\":   \"./models/laser/bilstm.93langs.2018-12-26.pt\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Instantiate encoder\n",
    "# BUG: CUDA GPU memory is exceeded if both laser and labse are loaded together \n",
    "# @clru_cache(None)\n",
    "def get_laser_model():\n",
    "    laser_model = Laser(\n",
    "        bpe_codes = config['laser']['bpe_codes'],\n",
    "        bpe_vocab = config['laser']['bpe_vocab'],\n",
    "        encoder   = config['laser']['encoder'],\n",
    "        tokenizer_options = None,\n",
    "        embedding_options = None\n",
    "    )\n",
    "    return laser_model\n",
    "def laser_encode(text: Union[str, List[str]], lang='en', normalize=True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encodes a corpus of text using LASER\n",
    "    :param text: Large block of text (will be tokenized), or list of pre-tokenized sentences\n",
    "    :param lang: 2 digit language code (optional autodetect)\n",
    "    :return:     embedding matrix\n",
    "    \"\"\"\n",
    "    laser_model = get_laser_model()\n",
    "    \n",
    "    # lang = lang or language_detect(text, threshold=0.0)\n",
    "    if isinstance(text, str):\n",
    "        # sentences = punkt_tokenize_sentences(text, lang=lang)\n",
    "        sentences = [ text ]\n",
    "    else:\n",
    "        sentences = list(text)\n",
    "\n",
    "    embedding = laser_model.embed_sentences(sentences, lang=lang)\n",
    "    \n",
    "    if normalize:\n",
    "        embedding = embedding / np.sqrt(np.sum(embedding**2, axis=1)).reshape(-1,1)\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71e5b778",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mucberkeley-dlab/measuring-hate-speech\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# import datasets \n",
    "# dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech', 'binary')   \n",
    "# df = dataset['train'].to_pandas()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "add1c5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        hate_speech_score                                               text  \\\n",
      "0                   -3.90  Yes indeed. She sort of reminds me of the elde...   \n",
      "1                   -6.52  The trans women reading this tweet right now i...   \n",
      "2                    0.36  Question: These 4 broads who criticize America...   \n",
      "3                    0.26  It is about time for all illegals to go back t...   \n",
      "4                    1.54  For starters bend over the one in pink and kic...   \n",
      "...                   ...                                                ...   \n",
      "135551              -4.88  Ø¹Ø§Ø¬Ù„ Ø³Ù…Ø§Ø­Ø© #Ø§Ù„Ø³ÙŠØ¯_Ø¹Ø¨Ø¯Ø§Ù„Ù…Ù„Ùƒ_Ø¨Ø¯Ø±Ø§Ù„Ø¯ÙŠÙ†_Ø§Ù„Ø­ÙˆØ«ÙŠ  Ù†Øµ...   \n",
      "135552              -4.40  Millions of #Yemen-is participated in mass ral...   \n",
      "135553              -2.49  @AbeShinzo @realDonaldTrump @shinzoabe ç‹¬è£è€…ã¯è¡Œãã¾...   \n",
      "135554              -4.40  Millions of #Yemen-is participated in mass ral...   \n",
      "135555              -0.20  Ù„Ø§ ØªØªØ´Ù…Øª Ø§Ù„Ø±Ø¬Ø§Ù„ Ù…Ø³ÙƒÙŠÙ† ÙŠØ¹Ø§Ù†ÙŠ ÙƒØ³ Ø§Ù…Ù‡ ðŸ˜‚. ÙŠÙ‚ÙˆÙ„ ÙŠØ§Ù„...   \n",
      "\n",
      "        target_race  target_religion  target_origin  target_gender  \\\n",
      "0              True            False          False          False   \n",
      "1             False            False          False           True   \n",
      "2             False            False           True          False   \n",
      "3             False            False           True          False   \n",
      "4             False            False          False           True   \n",
      "...             ...              ...            ...            ...   \n",
      "135551        False             True          False          False   \n",
      "135552         True             True          False          False   \n",
      "135553        False             True           True          False   \n",
      "135554        False             True          False          False   \n",
      "135555        False            False           True          False   \n",
      "\n",
      "        target_sexuality  target_age  target_disability  \n",
      "0                  False       False              False  \n",
      "1                  False       False              False  \n",
      "2                  False       False              False  \n",
      "3                  False       False              False  \n",
      "4                  False       False              False  \n",
      "...                  ...         ...                ...  \n",
      "135551             False       False              False  \n",
      "135552             False       False              False  \n",
      "135553             False       False              False  \n",
      "135554             False       False              False  \n",
      "135555             False       False              False  \n",
      "\n",
      "[135556 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# network.summary()\n",
    "df1 = pd.read_csv(\"dataset_multi.csv\", delimiter=\",\", encoding='utf-8')\n",
    "\n",
    "# print(col)\n",
    "l=['hate_speech_score','text','target_race','target_religion','target_origin','target_gender','target_sexuality','target_age','target_disability']\n",
    "df1=df1[l]\n",
    "col=list(df1.columns)\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1ec6d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        isHate                                            comment  \\\n",
      "4          1.0  For starters bend over the one in pink and kic...   \n",
      "7          1.0         Fuck off you insufferable retarded faggot.   \n",
      "14         1.0  White Small Little Invisible Clits Are A Disgr...   \n",
      "17         1.0  @suddenlywestan Because you are a country of m...   \n",
      "18         1.0  This sub is full of grade A faggots  Comedy fu...   \n",
      "...        ...                                                ...   \n",
      "122428     1.0  STOP SUCKING THAT FATASS' DICK, YOU NIGGERLOVI...   \n",
      "122429     1.0  Stupid Asian Bitch You're an idiot and you sho...   \n",
      "122430     1.0  Stupid Asian Bitch You're an idiot and you sho...   \n",
      "122431     1.0                         Fucking fagot burn in hell   \n",
      "122434     1.0  Niggers and their liberal friends steal everyt...   \n",
      "\n",
      "        target_race  target_religion  target_origin  target_gender  \\\n",
      "4             False            False          False           True   \n",
      "7             False            False          False          False   \n",
      "14             True            False          False           True   \n",
      "17            False            False           True          False   \n",
      "18            False            False          False          False   \n",
      "...             ...              ...            ...            ...   \n",
      "122428        False             True          False          False   \n",
      "122429         True            False          False          False   \n",
      "122430         True            False          False          False   \n",
      "122431        False            False          False          False   \n",
      "122434         True            False          False          False   \n",
      "\n",
      "        target_sexuality  target_age  target_disability  \n",
      "4                  False       False              False  \n",
      "7                   True       False              False  \n",
      "14                 False       False              False  \n",
      "17                 False       False               True  \n",
      "18                  True       False              False  \n",
      "...                  ...         ...                ...  \n",
      "122428             False       False              False  \n",
      "122429             False       False              False  \n",
      "122430             False       False              False  \n",
      "122431              True       False              False  \n",
      "122434             False       False              False  \n",
      "\n",
      "[49273 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df1.loc[df1['hate_speech_score'] >= 0.5,'hate_speech_score' ] = 1\n",
    "df1.loc[df1['hate_speech_score'] <0.5,'hate_speech_score'] = 0\n",
    "# df1=df1[['text','hate_speech_score']]\n",
    "# print(df1)\n",
    "df1.rename(columns={'text':'comment','hate_speech_score':'isHate'},inplace=True)\n",
    "df1=df1[df1['isHate']==1]\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f31f4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "(49273, 7)\n"
     ]
    }
   ],
   "source": [
    "col=col[2:]\n",
    "for i in col:\n",
    "    df1.loc[df1[i] == 'True', i] = 1.0\n",
    "    df1.loc[df1[i] == 'False', i] = 0.0\n",
    "#     df1[i]=df1[i].apply(pd.to_numeric)\n",
    "    df1[i] = df1[i].astype(int)\n",
    "col.append(\"comment\")\n",
    "df1=df1[col]\n",
    "df1=df1.reset_index(drop=True)\n",
    "\n",
    "# print(df1.comment)\n",
    "processed_tweets = preprocess(df1.comment)   \n",
    "df1['processed_tweets'] = processed_tweets\n",
    "df1 = df1.sample(frac = 1)\n",
    "# print(df1)\n",
    "word_tokenizer = Tokenizer()                      \n",
    "word_tokenizer.fit_on_texts(df1[\"processed_tweets\"])\n",
    "# print(list(df1[\"processed_tweets\"]))\n",
    "brown_tagged_words=list(df1[\"processed_tweets\"])\n",
    "brown_tagged_words_encoded =word_tokenizer.texts_to_sequences(brown_tagged_words)\n",
    "MAX_SEQ_LENGTH = max(lengths)\n",
    "brown_tagged_words_padded = pad_sequences(brown_tagged_words_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "print(len(brown_tagged_words_padded[0]))\n",
    "# brown_tagged_words_encoded = word_tokenizer.texts_to_sequences(df1[\"processed_tweets\"])\n",
    "col.remove('comment')\n",
    "df2=df1[col]\n",
    "# print(df2)\n",
    "brown_tagged_vect=df2.to_numpy()\n",
    "print(brown_tagged_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bf7abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49273 entries, 42477 to 15417\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   target_race        49273 non-null  int32 \n",
      " 1   target_religion    49273 non-null  int32 \n",
      " 2   target_origin      49273 non-null  int32 \n",
      " 3   target_gender      49273 non-null  int32 \n",
      " 4   target_sexuality   49273 non-null  int32 \n",
      " 5   target_age         49273 non-null  int32 \n",
      " 6   target_disability  49273 non-null  int32 \n",
      " 7   comment            49273 non-null  object\n",
      " 8   processed_tweets   49273 non-null  object\n",
      "dtypes: int32(7), object(2)\n",
      "memory usage: 2.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1691dd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence: 66\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(seq) for seq in brown_tagged_words_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))\n",
    "MAX_SEQ_LENGTH = max(lengths)\n",
    "# brown_tagged_words_padded = pad_sequences(brown_tagged_words_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
    "# print(len(brown_tagged_words_padded[0]))\n",
    "# print((word_tokenizer.word_index) )\n",
    "path = \"GoogleNews-vectors-negative300.bin\"\n",
    "word2vec= KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "EMBEDDING_SIZE  = 300\n",
    "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "word2id = word_tokenizer.word_index\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b3a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31534, 66) (31534, 7)\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 107s 2s/step - loss: 8.3838 - categorical_accuracy: 0.5887 - val_loss: 6.7529 - val_categorical_accuracy: 0.5828\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 93s 2s/step - loss: 6.1832 - categorical_accuracy: 0.6834 - val_loss: 4.5129 - val_categorical_accuracy: 0.6523\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 88s 1s/step - loss: 5.5478 - categorical_accuracy: 0.6934 - val_loss: 3.9151 - val_categorical_accuracy: 0.6877\n",
      "Epoch 4/10\n",
      "31/62 [==============>...............] - ETA: 42s - loss: 5.5223 - categorical_accuracy: 0.7068"
     ]
    }
   ],
   "source": [
    "len_brown_data=len(brown_tagged_words_padded)/5\n",
    "avg_loss=0.0\n",
    "avg_acc=0.0\n",
    "confusion_mat=[]\n",
    "per_pos_acc=[]\n",
    "Y_pred=[]\n",
    "Y_true=[]\n",
    "# temp = list(zip(brown_tagged_words_padded,brown_tagged_vect ))\n",
    "# random.shuffle(temp)\n",
    "# brown_tagged_words_padded,brown_tagged_vect= zip(*temp)\n",
    "# # res1 and res2 come out as tuples, and so must be converted to lists.\n",
    "# brown_tagged_words_padded,brown_tagged_vect = list(brown_tagged_words_padded), list(brown_tagged_vect)\n",
    "# print(brown_tagged_vect)\n",
    "# print(len(brown_tagged_words_padded))\n",
    "# for i in range(5):\n",
    "# first_part=int(i*(len_brown_data))\n",
    "# second_part=int((i+1)*len_brown_data)\n",
    "# X_train=np.concatenate((brown_tagged_words_padded[:first_part],brown_tagged_words_padded[second_part:]),axis=0)\n",
    "# X_test =brown_tagged_words_padded[first_part:second_part]\n",
    "# Y_train=np.concatenate((brown_tagged_vect[:first_part],brown_tagged_vect[second_part:]),axis=0)\n",
    "# Y_test =brown_tagged_vect[first_part:second_part]\n",
    "\n",
    "#     Y_test_enc=brown_tagged_tags_encoded[first_part:second_part]\n",
    "brown_tagged_words_padded = np.asarray(brown_tagged_words_padded).astype('float32')\n",
    "brown_tagged_vect = np.asarray(brown_tagged_vect).astype('float32')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(brown_tagged_words_padded, brown_tagged_vect, test_size=0.2, random_state=42)\n",
    "X_train,X_validation=train_test_split(X_train,train_size=0.80,test_size=0.20,random_state = 50)\n",
    "Y_train,Y_validation=train_test_split(Y_train,train_size=0.80,test_size=0.20,random_state = 50)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "#     print(X_validation.shape)\n",
    "network = models.Sequential()\n",
    "#     Embedding(input_dim     = VOCABULARY_SIZE,\n",
    "#                          output_dim    = EMBEDDING_SIZE,\n",
    "#                          input_length  = MAX_SEQ_LENGTH,\n",
    "# #                          weights       = [embedding_weights],\n",
    "#                          ),\n",
    "#     Bidirectional(LSTM(16)),\n",
    "    \n",
    "# #     tf.keras.Input(shape=(,)),\n",
    "#     tf.keras.layers.Dense(512, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dropout(0.25),\n",
    "#     tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dropout(0.25),\n",
    "#     tf.keras.layers.Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dropout(0.25),\n",
    "# #     tf.keras.layers.Dense(8, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "# #         tf.keras.layers.BatchNormalization(),\n",
    "# #         tf.keras.layers.Dropout(0.25),\n",
    "#     tf.keras.layers.Dense(8, activation=tf.keras.activations.sigmoid),\n",
    "# ]\n",
    "#     ) ## starting the ffnn\n",
    "network.add(Embedding(input_dim     = VOCABULARY_SIZE,\n",
    "                         output_dim    = EMBEDDING_SIZE,\n",
    "                         input_length  = MAX_SEQ_LENGTH,\n",
    "#                          weights       = [embedding_weights],\n",
    "                         ))\n",
    "network.add(Bidirectional(LSTM(16))) \n",
    "#     network.add(layers.Dense(units=100))\n",
    "#     network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "#     network.add(layers.Dense(units=1,activation='relu'))\n",
    "network.add(layers.Dense(512, kernel_regularizer='l1'))\n",
    "network.add(layers.BatchNormalization())\n",
    "network.add(layers.Dropout(0.3))\n",
    "network.add(layers.Dense(7, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4), # Root Mean Square Propagation\n",
    "            metrics=['categorical_accuracy'])\n",
    "trainng = network.fit(X_train,Y_train,batch_size=512,epochs=10,validation_data=(X_validation, Y_validation))\n",
    "loss,accuracy = network.evaluate(X_test,Y_test,verbose=1)\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "avg_loss=avg_loss+loss\n",
    "avg_acc=avg_acc+accuracy\n",
    "pred=network.predict(X_test)\n",
    "print(pred)\n",
    "print(Y_test)\n",
    "# print(avg_acc/5)\n",
    "#     print(avg_acc)\n",
    "#     indices=np.argmax(pred, axis=2)\n",
    "    #not_zero=indices[indices != 0]\n",
    "#     Y_pred=indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_tokenizer.word_index['UNK'])\n",
    "sentence=input()\n",
    "# print(word_tokenizer.word_index)\n",
    "\n",
    "def sen_vec(sent):\n",
    "    sent=preprocess(sent)\n",
    "    sent=str(sent[0])\n",
    "    print(sent)\n",
    "    y=len(word_tokenizer.word_index)\n",
    "    words=sent.split(\" \")\n",
    "    print(words)\n",
    "    l = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in word_tokenizer.word_index:\n",
    "            l.append(word_tokenizer.word_index[words[i]])\n",
    "        else:\n",
    "            print(words[i])\n",
    "            try:\n",
    "                sim_word = word2vec.most_similar(positive=words[i],topn=10)[:]\n",
    "                print(sim_word)\n",
    "                x=0\n",
    "                for i in range(10):\n",
    "                    x=x+1\n",
    "                    if sim_word[i][0] in word_tokenizer.word_index:\n",
    "                        l.append(word_tokenizer.word_index[sim_word[i][0]])\n",
    "                        break\n",
    "                if(x==10):\n",
    "#                     print(\"y is not in the vocalbulary\")\n",
    "#                     print(\"y is not in the vocalbulary\")\n",
    "                        l.append(0)\n",
    "            \n",
    "            except KeyError:\n",
    "#                 sys.exit(\"y is not in the vocalbulary\")\n",
    "                \n",
    "#                 y=y+1\n",
    "                l.append(0)\n",
    "                \n",
    "            \n",
    "#         x=0\n",
    "#         for j in range(len(brown_tagged_words_encoded)):\n",
    "#             for k in range(len(brown_tagged_words_encoded[j])):\n",
    "#                 if(x==0 and words[i]==brown_tagged_words[j][k]):\n",
    "#                     l.append(int(brown_tagged_words_encoded[j][k]))\n",
    "#                     x=1\n",
    "#                     break\n",
    "#             if(x==1):\n",
    "#                 break\n",
    "#         if(x==0):\n",
    "#             sim_word = word2vec.most_similar(positive=words[i],topn=10)[:][0]\n",
    "#             while(x==0):\n",
    "#                 for j in range(len(brown_tagged_words_encoded)):\n",
    "#                     for k in range(len(brown_tagged_words_encoded[j])):\n",
    "#                         if(x==0 and words[i]==brown_tagged_words[j][k]):\n",
    "#                             l.append(int(brown_tagged_words_encoded[j][k]))\n",
    "#                             x=1\n",
    "#                             break\n",
    "#                     if(x==1):\n",
    "#                         break\n",
    "        \n",
    "              \n",
    "                            \n",
    "                        \n",
    "                \n",
    "    t=tf.convert_to_tensor(l)\n",
    "    t=tf.reshape(t, [1, t.shape[0]])\n",
    "    y = pad_sequences(t, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10fa1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "# sentence=sentence\n",
    "df = pd.DataFrame(columns=['sentence'])\n",
    "value1 = \"dog\"\n",
    "df = df.append({'sentence': sentence}, ignore_index=True)\n",
    "# from io import StringIO\n",
    "# StringData = StringIO(sentence)\n",
    " \n",
    "# # let's read the data using the Pandas\n",
    "# # read_csv() function\n",
    "# df = pd.read_csv(StringData, sep =\";\")\n",
    " \n",
    "# Print the dataframe\n",
    "# print(df)\n",
    "# print(df.sentence)\n",
    "l=sen_vec(df.sentence)\n",
    "print(\"yes\")\n",
    "pred=network.predict(l)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['processed_tweets'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161198ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf\n",
    "# tfidf_a = tfidf.toarray()\n",
    "print(tfidf_a)\n",
    "y = df['isHate'].astype(int)\n",
    "# X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "# X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "xgb = XGBClassifier()\n",
    "# %%time\n",
    "accuracy = []\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X, y)\n",
    "for clf in (lr,rf,xgb):\n",
    "\n",
    "    StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "            \n",
    "            X1_train, X1_test = X[train_index], X[test_index]\n",
    "            y1_train, y1_test = y[train_index], y[test_index]\n",
    "\n",
    "            model = clf.fit(X1_train,y1_train)\n",
    "            y_preds = model.predict(X1_test)\n",
    "            acc = (sklearn.metrics.f1_score(y1_test,y_preds,average = 'micro'))\n",
    "            accuracy.append(acc)\n",
    "    print(clf.__class__.__name__,np.array(accuracy).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b999707",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=LogisticRegression()\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "rf.fit(X_train_tfidf,y_train)\n",
    "y_preds = rf.predict(X_test_tfidf)\n",
    "acc1=accuracy_score(y_test,y_preds)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(y_test)\n",
    "print(y_preds)\n",
    "print(report)\n",
    "print(\"Random Forest, Accuracy Score:\",acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc6df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
